** ceph
*** 常用命令
**** ceph 运维常用指令
  	    - http://zhanguo1110.blog.51cto.com/5750817/1543032
        - http://linuxnote.blog.51cto.com/9876511/1788361
        - CEPH 集群 RBD 快照创建、恢复、删除、克隆
 	      http://andyliu.blog.51cto.com/518879/1856669

*** 集群
    1. 启动 mon 进程 service ceph start  mon.node1
    2. 启动 msd 进程 service ceph start mds.node1
    3. 启动 osd 进程 service ceph start osd.0
    4. 查看机器的监控状态 ceph health
    5. 查看 ceph 的实时运行状态: ceph -w
    6. 检查信息状态信息: ceph -s
    7. 查看 ceph 存储空间 ceph df
    8. 删除一个节点的所有的 ceph 数据包: ceph-deploy purge node1
    9. 为 ceph 创建一个 admin 用户并为 admin 用户创建一个密钥，把密钥保存到/etc/ceph 目
       录下：ceph auth get-or-create client.admin mds 'allow' osd 'allow *' mon
       'allow *' > /etc/ceph/ceph.client.admin.keyring 或 ceph auth get-or-create client.admin mds 'allow' osd 'allow *' mon 'allow *' -o /etc/ceph/ceph.client.admin.keyring
    10. 为 osd.0 创建一个用户并创建一个 key: ceph auth get-or-create osd.0 mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/osd/ceph-0/keyring
    11. 为 mds.node1 创建一个用户并创建一个 key: ceph auth get-or-create mds.node1 mon 'allow rwx' osd 'allow *' mds 'allow *' -o /var/lib/ceph/mds/ceph-node1/keyring
    12. 查看 ceph 集群中的认证用户及相关的 key: ceph auth list
    13. 删除集群中的一个认证用户: ceph auth del osd.0
    14. 查看集群的详细配置: ceph daemon mon.node1 config show | more
    15. 查看集群健康状态细节: ceph health detail
    16. 查看 ceph log 日志所在的目录:  ceph-conf --name mon.node1 --show-config-value log_file ceph-conf --name mon.node1 --show-config-value log_file

*** rbd 相关

**** "rbd 的增量备份和恢复 | zphj1987'Blog"
 	   http://www.zphj1987.com/2016/06/22/rbd%E7%9A%84%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/

**** ceph-rbd 常用命令
 	  https://ztjlovejava.github.io/2015/03/29/Ceph-rbd-cmd/

**** rbd 在线扩容
 	  https://my.oschina.net/oscfox/blog/312220

    #+BEGIN_SRC sh
      # rbd create --size 10000 test
      # rbd map test
      # mkfs.ext4 -q /dev/rbd1
      # mount /dev/rbd1 /mnt
      # df -h /mnt
      Filesystem      Size  Used Avail Use% Mounted on
      /dev/rbd1       9.5G   22M  9.0G   1% /mnt
      # blockdev --getsize64 /dev/rbd1
      10485760000
      # rbd resize --size 20000 test
      Resizing image: 100% complete...done.
      # blockdev --getsize64 /dev/rbd1
      20971520000
      # resize2fs /dev/rbd1
      resize2fs 1.42 (29-Nov-2011)
      Filesystem at /dev/rbd1 is mounted on /mnt; on-line resizing required
      old_desc_blocks = 1, new_desc_blocks = 2
      The filesystem on /dev/rbd1 is now 5120000 blocks long.
      # df -h /mnt
      Filesystem      Size  Used Avail Use% Mounted on
      /dev/rbd1        20G   27M   19G   1% /mnt
    #+END_SRC
    此方法只对 EXT4 文件系统块设备有效，XFS 文件系统，在 reset 之后执行 xfs_growfs /mnt
**** ceph backup
***** "How to Backup Ceph Block Storage
 	    https://nicksabine.com/post/ceph-backup/
***** "10. 查看使用 RBD 镜像的客户端 · Ceph 运维手册"
 	    https://lihaijing.gitbooks.io/ceph-handbook/content/Advance_usage/list_rbd_watcher.html
**** rbd format1 VS format2
     + Format 1: Hammer 以及 Hammer 之前默认都是这种格式，并且 rbd_default_features
       = 3., 不支持 rbd 的 clone 等功能
     + Format 2: Jewel 默认是这种格式，并且 rbd_default_features = 61.
*** ceph 配置相关
**** kubernetes 使用 ceph
 	   http://www.cnblogs.com/boshen-hzb/p/6673134.html
**** ceph 相关基本概念
*****  麦子迈|解析 Ceph : OSD , OSDMap 和 PG, PGMap
 	    http://www.wzxue.com/ceph-osd-and-pg/
      1. Monitor 作为 Ceph 的 Metada Server 维护了集群的信息，它包括了 6 个 Map，分别
         是 MONMap，OSDMap，PGMap，LogMap，AuthMap，MDSMap。其中 PGMap 和 OSDMap
         是最重要的两张 Map，会在本文主要涉及。
***** "在 ceph 中：pool、PG、OSD 的关系 - 波神 - 博客园"
 	     http://www.cnblogs.com/boshen-hzb/p/7169143.html
*** 常见问题以及解决方法
**** "删除 Ceph 的 image 报 rbd: error: image still has watchers - 波神 - 博客园"
 	   http://www.cnblogs.com/boshen-hzb/p/6756484.html
**** 常用命令
     fuser -awv dev/rbdx 查看当前设备文件被谁在占用
