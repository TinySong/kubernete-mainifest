* storageclass/pv/pvc
** PV & PVC
*** 介绍
    PersistentVolume（pv）和 PersistentVolumeClaim（pvc）是 k8s 提供的两种 API
    资 源，用于抽象存储细节。管理员关注于如何通过 pv 提供存储功能而无需关注用户
    如何 使用，同样的用户只需要挂载 pvc 到容器中而不需要关注存储卷采用何种技术实
    现。pvc 和 pv 的关系与 pod 和 node 关系类似，前者消耗后者的资源。pvc 可以向
    pv 申 请指定大小的存储资源并设置访问模式,这就可以通过 Provision -> Claim 的
    方式，来对存储资源进行控制。
*** 声明周期
    + 供应准备 通过集群外的存储系统或者云平台来提供存储持久化支持。
      - 静态提供：管理员手动创建多个 PV，供 PVC 使用。
      - 动态提供：动态创建 PVC 特定的 PV，并绑定。 (通过 storageClasses 实现) + 绑
      定。用户创建 pvc 并指定需要的资源和访问模式。在找到可用 pv 之前，pvc 会保
      持未绑定状态。
    + 使用。用户可在 pod 中像 volume 一样使用 pvc。
    + 释放。用户删除 pvc 来回收存储资源，pv 将变成“released”状态。由于还保留着
      之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他
      pvc 使用。
    + 回收(Reclaiming)。pv 可以设置三种回收策略：保留（Retain），回收（Recycle）
      和删除（Delete）
      - 保留策略：允许人工处理保留的数据。
      - 删除策略：将删除 pv 和外部关联的存储资源，需要插件支持。
      - 回收策略：将执行清除操作，之后可以被新的 pvc 使用，需要插件支持。 目前只
       有 NFS 和 HostPath 类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk 和
       Cinder 支持删除(Delete)策略。claims must exist in the same namespaces as
       the pod using the claim claims & volumes

       #+BEGIN_SRC yaml
            kind: Pod
            apiVersion: v1
            metadata:
              name: mypod
            spec:
              containers:
                - name: myfrontend
                  image: dockerfile/nginx
                  volumeMounts:
                  - mountPath: "/var/www/html"
                    name: mypd
              volumes:
                - name: mypd
                  persistentVolumeClaim:
                    claimName: myclaim
       #+END_SRC

*** 实验操作
    首先创建 pv， 然后创建 pvc， 然后创建 pod 时，通过 volume 的挂载方式实现挂载
    卷, 这个例子是使用的的 volume 是 host 模式，所以当创建完 pod 后，会在运行
    pod 的 node 节点上存在/tmp/data 目录，这时你通过*echo 'Hello from Kubernetes
    storage' > /tmp/data/index.html*方式在／tmp/data 目录下创建一个 index.html,
    然后进入到 container 中，安装 curl 命令， 执行 curl localhost，则会返回
    index.html 中的内容
**** 创建 pv
        the configuration file specifies that the volume is at /tmp/data on the the
       cluster’s Node. The configuration also specifies a size of 10 gibibytes
       and an access mode of ReadWriteOnce, which means the volume can be
       mounted as read-write by a single Node. It defines the StorageClass name
       manual for the PersistentVolume, which will be used to bind
       PersistentVolumeClaim requests to this PersistentVolume.
       #+BEGIN_SRC yaml
        kind: PersistentVolume
        apiVersion: v1
        metadata:
          name: task-pv-volume
          labels:
            type: local
        spec:
          storageClassName: manual
          capacity:
            storage: 10Gi
          accessModes:
            - ReadWriteOnce
          hostPath:
            path: "/tmp/data"
       #+END_SRC
**** 创建 pvc
     After you create the PersistentVolumeClaim, the Kubernetes control plane
       looks for a PersistentVolume that satisfies the claim’s requirements. If
       the control plane finds a suitable PersistentVolume with the same
       StorageClass, it binds the claim to thevolume.
       #+BEGIN_SRC yaml
         kind: PersistentVolumeClaim
         apiVersion: v1
         metadata:
           name: task-pv-claim
         spec:
           storageClassName: manual
           accessModes:
             - ReadWriteOnce
           resources:
             requests:
               storage: 3Gi
       #+END_SRC
**** 创建 pod
     #+BEGIN_SRC yaml
       kind: Pod
       apiVersion: v1
       metadata:
         name: task-pv-pod
       spec:

         volumes:
           - name: task-pv-storage
             persistentVolumeClaim:
              claimName: task-pv-claim

         containers:
           - name: task-pv-container
             image: nginx
             ports:
               - containerPort: 80
                 name: "http-server"
             volumeMounts:
             - mountPath: "/usr/share/nginx/html"
               name: task-pv-storage

     #+END_SRC
*** updateSize
    + pvc cannot update requests->storage
    + pv can update capacity->storage, and automatic update pvc size

** storageclass
*** Change the default StorageClass
 	  https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/
** statefulset
*** pvc will not delete when statefulset be deleted
    Deleting and/or scaling a StatefulSet down will not delete the volumes
   associated with the StatefulSet. This is done to ensure data safety, which is
   generally more valuable than an automatic purge of all related StatefulSet
   resources.
*** headless service be required
    *A Headless Service, named nginx, is used to control the network domain.*
*** Note
    *Note that, the PersistentVolumes associated with the Pods’*
    *PersistentVolume Claims are not deleted when the Pods, or StatefulSet are*
    *deleted. This must be done manually.*
*** OrderedReady Pod Management
    orderedReady pod management is the default for statefulsets.
*** update strategy
**** on delete
**** rooling updates
     .spec.updateStrategy.type is set to RollingUpdate
*** 原理
    StatefulSet 由 Service 和 volumeClaimTemplates 组成。Service 中的多个 Pod 将会被分别
    编号，并挂载 volumeClaimTemplates 中声明的 PV。
*** 参考阅读
**** Kubernetes 如何支持有状态服务的部署
 	  http://www.cnblogs.com/Jack47/p/deploy-stateful-application-on-Kubernetes.html

** storageclass 使用到项目中
*** kube-controller-manager 和 kubelet 的容器或主机 中需要集成 rbd 命令
    + Volume Provisioning: Currently, if you want dynamic provisioning, RBD
      provisioner in controller-manager needs to access rbd binary to create new
      image in ceph cluster for your PVC.external-storage plans to move volume
      provisioners from in-tree to out-of-tree, there will be a separated RBD
      provisioner container image with rbd utility included
      (kubernetes-incubator/external-storage#200), then controller-manager do
      not need access rbd binary anymore.
    + Volume Attach/Detach: kubelet needs to access rbd binary to attach (rbd
      map) and detach (rbd unmap) RBD image on node. If kubelet is running on the
      host, hostneeds to install rbd utility (install ceph-common package on
      most Linux distributions).
    *请参考：https://github.com/kubernetes/kubernetes/issues/38923*
*** storageclass 使用时注意事项
    1. 在使用 storageclass 方式动态创建 pv 和 pvc 时，需要其 namespaces 中创建一个 secret，
       这个 secret 是通过 ceph auth list 中的 key 通过 base64 计算的到
    2. 若 pod 中的 volume 使用的 pvc 是通过 storageclass 创建的来，那么就需要在 pod 所在通
       过存在 secret 和 storageclass,否则 pod mount 不上 pvc
    3. kube-controller-manager 和 kubelet 的容器或主机 中需要集成 rbd 命令
     *请参考：https://github.com/kubernetes/kubernetes/issues/38923*


*** storageclass 在 statefulset 中的应用
**** 创建 statefulset 注意点
    1. 在 statefulset 所在的 ns 中存在连接 ceph 的 secret 配置信息
    2. 在 statefluset 所在的 ns 中存在连接 ceph 的 storageclass 配置信息
    3. 在创建的 statefulset 时，通过 volumeClaimtemplates->storageClassName: xxxx，
       指定在哪个 storageclass 上自动创建 pvc 和 pv
    4. 当 statefulSet 中的 accessModes 为 ReadWriteMany 时，每个 node 节点上只能有一个实例，
       当为 ReadOnlyMany 时，可多个实例运行在同一个 node 节点上，当为 ReadWriteOnce 时，
       可多个实例运行在同一个 node 节点上

**** 实例演示
     1. 创建 secret key(每个命名空间一个)
       通过 ceph  auth list 可查看到所有的 ceph 已经创建的用户以及认证信息,由于
      Kubernetes 的 Secret 需要 Base64 编码，下面将这个 keyring 转换成 Base64 编
      码,eg，将 client.admin 的 key 转换为 base64 命令：ceph auth get-key
      client.admin | base64, 然后将输出的 key 写入 secret.yaml 配置文件，如：

      #+BEGIN_SRC yaml
      apiVersion: v1
      kind: Secret
      metadata:
        name: ceph-secret
        #namespace: kube-system
      type: kubernetes.io/rbd  #非常重要，如果想让 storageclass 识别必须加这个，文档示例上没写，但是 example 里写了
      data:
        key: QVFBOW1VTlpGUjVlQ2hBQXFGbEgyS0M3c2Zqakx4QjNmUFJUd0E9PQ== #创建命令 ceph auth get-key client.admin | base64
      #+END_SRC
      *注意*：It must exist in the same namespace as PVCs
     2. storageclass 创建 (无命名空间区分 storageclass is not namespaced)
        一个 storageclass，多个命名空间都可以使用，storageclass yaml 配置文件示例：

        #+BEGIN_SRC yaml
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: tenx-rbd
            # annotations:
                # storageclass.kubernetes.io/is-default-class: "true"    # 表示这个 StorageClass 是集群默认的 StorageClass
            labels:
              kubernetes.io/cluster-service: "true"
          provisioner: kubernetes.io/rbd            # 表示这个 StorageClass 的类型时 Ceph RBD
          parameters:                               # 配置了这个 StorageClass 使用的 Ceph 集群以及 RBD 的相关参数
            monitors: 192.168.0.68:6789,192.168.0.55:6789,192.168.0.94:6789,192.168.0.99:6789 #逗号分隔的 Ceph Mon 节点地址
            adminId: admin                 # 指定 Ceph client 的 ID 需要具有能在配置的 Ceph RBD Pool 中创建镜像的权限。默认值为 admin
            adminSecretName: ceph-secret   # adminId 的 Secret Name,该 Secret 的 type 必须是”kubernetes.io/rbd”，该参数是必须的
            adminSecretNamespace: "kube-system"  #TODO:作用 adminSecret 的 namespace，默认为”default”,
            pool: tenx-pool   # Ceph RBD Pool，默认为”rbd”
            userId: admin     # Ceph client Id，用来映射 RBD 镜像
            userSecretName: ceph-secret # The name of Ceph Secret for userId to map RBD image. It must exist in the same namespace as PVCs
        #+END_SRC
     3. statefulset 的创建
        + statefulset 重点在于 volumeClaimTemplates 中的 accessModes, 和 storageCalssName, accessModes 见
         https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
         ，storageClassName 就是动态创建 pv 和 pvc 的的 storageclass 的名字, 通过
         statefulset 动态创建 pv 和 pvc 的方式，pv 的 RECLAIMPOLICY 为 DELETE,若修改，
         需要手动修改 RECLAIMPOLICY，命令：
         #+BEGIN_SRC sh
           kubectl patch pv <your-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'，
         #+END_SRC
        + mysqlstatefulset 示例：

          #+BEGIN_SRC yaml
             apiVersion: v1
             kind: Service
             metadata:
               annotations:
                 tenxcloud.com/schemaPortname: mysqltest/TCP
                 system/lbgroup: none
               name: mysqltest-pgytt
               labels:
                 app: mysqltest-pgytt
             spec:
               ports:
                 - port: 3306
                   name: mysqltest
               selector:
                 app: mysqltest
               externalIPs:
                 - 11.11.1.1
             ---
             apiVersion: v1
             kind: Service
             metadata:
               annotations:
                 service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
               name: mysqltest
               labels:
                 app: mysqltest
             spec:
               ports:
               - port: 3306
                 name: mysqltest
               clusterIP: None
               selector:
                 app: mysqltest
             ---
             apiVersion: apps/v1beta1
             kind: StatefulSet
             metadata:
               name: mysqltest
               namespace: kube-system
             spec:
               serviceName: mysqltest
               replicas: 2
               template:
                 metadata:
                   labels:
                     app: mysqltest
                   annotations:
                     pod.alpha.kubernetes.io/initialized: "true"
                     pod.alpha.kubernetes.io/init-containers: '[
                         {
                             "name": "install",
                             "image": "192.168.1.52/tenx_containers/galera-install:utf8",
                             "imagePullPolicy": "Always",
                             "args": ["--work-dir=/work-dir"],
                             "volumeMounts": [
                                 {
                                     "name": "workdir",
                                     "mountPath": "/work-dir"
                                 },
                                 {
                                     "name": "config",
                                     "mountPath": "/etc/mysql"
                                 }
                             ]
                         },
                         {
                             "name": "bootstrap",
                             "image": "192.168.1.52/tenx_containers/debian:jessie",
                             "command": ["/work-dir/peer-finder"],
                             "args": ["-on-start=\"/work-dir/on-start.sh\"", "-service=mysqltest"],
                             "env": [
                               {
                                   "name": "POD_NAMESPACE",
                                   "valueFrom": {
                                       "fieldRef": {
                                           "apiVersion": "v1",
                                           "fieldPath": "metadata.namespace"
                                       }
                                   }
                                }
                             ],
                             "volumeMounts": [
                                 {
                                     "name": "workdir",
                                     "mountPath": "/work-dir"
                                 },
                                 {
                                     "name": "config",
                                     "mountPath": "/etc/mysql"
                                 }
                             ]
                         }
                     ]'
                 spec:
                   terminationGracePeriodSeconds: 0
                   containers:
                   - name: mysqltest
                   image: 192.168.1.52/tenx_containers/mysql-galera:e2e
                    ports:
                    - containerPort: 3306
                      name: mysqltest
                    - containerPort: 4444
                      name: sst
                    - containerPort: 4567
                      name: replication
                    - containerPort: 4568
                      name: ist
                    env:
                    - name: MYSQL_ROOT_PASSWORD
                      value: "123123"
                    resources:
                      limits:
                        memory: '512Mi'
                      requests:
                        cpu: 50m
                        memory: '512Mi'
                    args:
                    - --defaults-file=/etc/mysql/my-galera.cnf
                    - --user=root
                    volumeMounts:
                    - name: datadir
                      mountPath: /var/lib/
                    - name: config
                      mountPath: /etc/mysql
                  volumes:
                  - name: config
                    emptyDir: {}
                  - name: workdir
                    emptyDir: {}
              volumeClaimTemplates:
              - metadata:
                  name: datadir
                spec:
                  accessModes: [ "ReadWriteMany" ]
                  storageClassName: tenx-rbd
                  resources:
                    requests:
                      storage: 512Mi
          #+END_SRC

**** statefulset 集成 storageclass
     每个命名空间下必须有一个 ceph 的 secret，需要与 pvc 在相同的命名空间，可存在
     多个 storageclass，
*****  创建集群
      1. 页面参数：副本数、存储大小、密码、是否定时备份、备份间隔时间
      2. 检查 secret 是否存在，存在进行下一步，不存在，创建 ceph 认证 secret,
         (secret 中的 key 是将 ceph 认证用户的 key 进行 base64 编码得到,command:
       ceph auth get-key client.admin | base64)
      3. 是否需要定时备份，以及备份时间间隔， 若需要定时备份，则启动 goroutine 进
         行定时备份，默认有一个时间间隔（1 天）,定时备份机制可采用定时创建快照的
         方式进行备份
      4. 根据前端传递的 statefulset/service/headlessService 配置，调用 kubernetes
         API 分别创建 statefulset/service/headlessService, 同时会自动创建 pvc 和 pv，
         pvc 的名字命名由三部分组成为：<volumeClaimTemplates:name>-<statefulsetName>-x

***** 删除集群
      1. 删除 statefulset 信息
      2. 检查参数是否要同时删除存储，存储是否保留, 不保留则删除 statefulset 对应的
         pvc 即可，pv 自动删除(pvc 的名字命名规则为：
         volumeClaimTemplatesName-statefulsetName-X, X 为数字)
      3. 删除 headlessService 和 service 信息

***** 集群扩容
      目前 stroageclass 不支持自动扩容，需要手动完成,分两个步骤：修改 rbd images,
      修改 pv
      1. rbd image 扩容
        扩容可以使用 rbd-storage-aent 流程，扩容 rbd 的 image 大小
      2. 通过 pvc 配置信息 找到 相应的 pv，修改 pv 中 capacity->storage 域的大小,
         pvc storage 大小会自动同步
      3. 后台暂定 statefulSet 服务，修改 statefulSet 中 storage 大小，然后重新启
         动 statefulSet 服务，之前的存储卷会自动匹配上，之后进行水平扩展时自动使用
         新的 storage 大小
      *注意 statefulSet 中的 requests->storage 无法在线修改，当需要统计磁盘使用情况时，
      不要通过此字段进行统计，需要相应的 pvc 进行统计*

***** 集群水平扩展
      可使用 kubectl scale 或 kubectl patch 水平扩展集群,水平扩展时，新创建的
      pvc 按照 statefulSet 中的 storage 的大小新建 pv,实例缩减时，pvc/pv 不会自动删除，
      当缩减时，需要手动删除相应的 pvc/pv
      1. kubectl scale 水平扩展或水平缩减都可以
         #+BEGIN_SRC sh
          kubectl scale  statefulsets/mysqltest -n kube-system --replicas=2
         #+END_SRC
      2.  kubectl patch 水平扩展或水平缩减
         #+BEGIN_SRC sh
           kubectl patch statefulsets/mysqltest -n kube-system  -p '{"spec":{"replicas":3}}'
         #+END_SRC

***** statefulSet 创建数据块复用
      使用场景：statefulSet 服务删除时，保留数据块，新建服务时，想服用原来的数据
      注意： 数据可以复用但条件比较苛刻
       1，复用时创建数据库服务的名字要与之前一致
       2. 命名空间要一致，
       3. volumeClaimTemplates 中 name 名字要相同)

*** storagelcass 在 deployment 中的应用
    在 deployment 的应用一般时已经创建好了的 pvc-pv，只需要在 deployment yaml 配置
    的 volumes 域 persistentVolumeClaim->claimName 制定 pvc 名字即可使用
**** pvc 的创建（及创建存储）
     当前 pvc 的创建也是基于 storageclass 实现，创建一个名字为 task-pv-claim 的 pvc，yaml 示例：
     #+BEGIN_SRC yaml
       kind: PersistentVolumeClaim
       apiVersion: v1
       metadata:
         name: task-pv-claim
         namespace: default
       spec:
         accessModes:
           - ReadWriteOnce
         storageClassName: tenx-rbd
         resources:
           requests:
             storage: 3Gi
     #+END_SRC
     pvc 的 ns 为 default，这时则可以看到 pvc 已经创建成功，并且已经 bound 上 pv 了

     #+BEGIN_SRC sh
       $song in storage  on master ● λ kubectl get pvc
       kuNAME                     STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
       task-pv-claim            Bound     pvc-1a431d74-8226-11e7-bd70-005056850b72   3Gi        RWO           tenx-rbd       1m
       $song in storage  on master ● λ kubectl describe pvc task-pv-claim
       Name:		task-pv-claim
       Namespace:	default
       StorageClass:	tenx-rbd
       Status:		Bound
       Volume:		pvc-1a431d74-8226-11e7-bd70-005056850b72
       Labels:		<none>
       Annotations:	pv.kubernetes.io/bind-completed=yes
           pv.kubernetes.io/bound-by-controller=yes
           volume.beta.kubernetes.io/storage-provisioner=kubernetes.io/rbd
       Capacity:	3Gi
       Access Modes:	RWO
       Events:
         FirstSeen	LastSeen	Count	From				SubObjectPath	Type		Reason			Message
         ---------	--------	-----	----				-------------	--------	------			-------
         1m		1m		1	persistentvolume-controller			Normal		ProvisioningSucceeded	Successfully provisioned volume pvc-1a431d74-8226-11e7-bd70-005056850b72 using kubernetes.io/rbd

     #+END_SRC
**** 创建 deployment
     这里以 pod 为例, yaml 配置为：
     #+BEGIN_SRC yaml
       kind: Pod
       apiVersion: v1
       metadata:
         name: task-pv-pod
       spec:

         volumes:
           - name: task-pv-storage
             persistentVolumeClaim:
              claimName: task-pv-claim
         containers:
           - name: task-pv-container
             image: nginx
             ports:
               - containerPort: 80
                 name: "http-server"
             volumeMounts:
             - mountPath: "/usr/share/nginx/html"
               name: task-pv-storage
     #+END_SRC
     这里创建了一个名字为 task-pv-pod 的 pod，通过 persistentVolumeClaim->claimName
     指定上一步骤中创建的 pvc，即可挂在成功, 注意 namespace 必须要与 pvc 相同
**** storageclass deployment 同一数据卷，多个实例同时进行读写操作模型
***** 一个数据卷一个实例写，多个实例读
      1. 创建同时具有 ReadWriteOnce、ReadOnlyMany 多个 accessMode 的 pvc，yaml 配置如：

         #+BEGIN_SRC yaml
           kind: PersistentVolumeClaim
           apiVersion: v1
           metadata:
             name: task-pv-claim
             namespace: default
           spec:
             accessModes:
               - ReadWriteOnce
               - ReadOnlyMany
             storageClassName: tenx-rbd
             resources:
               requests:
                 storage: 3Gi
         #+END_SRC

      2. 创建可读写 task-pv-claim 卷的 pod
         *注意当挂载具有多个 accessMode 的 pod 时，readOnly 必须存在*
         #+BEGIN_SRC yaml
           kind: Pod
           apiVersion: v1
           metadata:
             name: task-pv-pod-rw
           spec:

             volumes:
               - name: task-pv-storage
                 persistentVolumeClaim:
                   claimName: task-pv-claim
                   readOnly: false        # fase: rw, true: readOnly
             containers:
               - name: task-pv-container
                 image: nginx
                 ports:
                   - containerPort: 80
                     name: "http-server"
                 volumeMounts:
                 - mountPath: "/usr/share/nginx/html"
                   name: task-pv-storage
         #+END_SRC

      3. 创建只可读 task-pv-claim 卷的 pod
         *readOnly 为 true*
         #+BEGIN_SRC yaml
           kind: Pod
           apiVersion: v1
           metadata:
             name: task-pv-pod-readonly
           spec:

             volumes:
               - name: task-pv-storage
                 persistentVolumeClaim:
                   claimName: task-pv-claim
                   readOnly: true       # fase: rw, true: readOnly
             containers:
               - name: task-pv-container
                 image: nginx
                 ports:
                   - containerPort: 80
                     name: "http-server"
                 volumeMounts:
                 - mountPath: "/usr/share/nginx/html"
                   name: task-pv-storage
         #+END_SRC

         此时即可实现了对于一个卷，一个 pod 可读写，另一个 pod 只可读取的模型


    

**** 注意点
     只要采用了 stroageclass 的存储方式，就需要 secret，在与 pvc 和 pod 同 ns 下，要有相
     应的 secret

*** storageclass 磁盘扩容
    目前 kubernets 不支持自动扩容磁盘功能，需要进行手工操作，其中包括：1）手动更改
    pv 大小,pvc 自动更新容量大小 2）手动更改挂在的磁盘大小(以 ceph 为例，需要更改挂在磁盘的大小)

**** ceph 块设备
***** ext4 文件系统块设备扩容
      以上例中基于 ceph 的 storageclass 创建的 deployment 为例：
****** 准备工作
******* 查看 pvc 的情况
        上例中创建的 pvc 为 task-pv-claim
       #+BEGIN_SRC sh
         $song in storage  on master ● ● λ kubectl describe pvc task-pv-claim
         Name:		task-pv-claim
         Namespace:	default
         StorageClass:	tenx-rbd
         Status:		Bound
         Volume:		pvc-1a431d74-8226-11e7-bd70-005056850b72
         Labels:		<none>
         Annotations:	pv.kubernetes.io/bind-completed=yes
         pv.kubernetes.io/bound-by-controller=yes
         volume.beta.kubernetes.io/storage-provisioner=kubernetes.io/rbd
         Capacity:	3Gi
         Access Modes:	RWO
         Events:
         FirstSeen	LastSeen	Count	From				SubObjectPath	Type		Reason			Message
         ---------	--------	-----	----				-------------	--------	------			-------
         51m		51m		1	persistentvolume-controller			Normal		ProvisioningSucceeded	Successfully provisioned volume pvc-1a431d74-8226-11e7-bd70-005056850b72 using kubernetes.io/rbd
       #+END_SRC
       可看出 pvc 挂在 volume pv 为：pvc-1a431d74-8226-11e7-bd70-005056850b72, 状
       态为 Bound, StorageClass 为 tenx-rbd, 然后查看 pv 具体挂在的那个 rbd 块
******* 查看 pv 的情况

        #+BEGIN_SRC sh
          $song in storage  on master ● ● λ kubectl describe pv pvc-1a431d74-8226-11e7-bd70-005056850b72
          Name:		pvc-1a431d74-8226-11e7-bd70-005056850b72
          Labels:		<none>
          Annotations:	pv.kubernetes.io/bound-by-controller=yes
          pv.kubernetes.io/provisioned-by=kubernetes.io/rbd
          StorageClass:	tenx-rbd
          Status:		Bound
          Claim:		default/task-pv-claim
          Reclaim Policy:	Delete
          Access Modes:	RWO
          Capacity:	3Gi
          Message:
          Source:
          Type:		RBD (a Rados Block Device mount on the host that shares a pod's lifetime)
              CephMonitors:	[192.168.0.68:6789 192.168.0.55:6789 192.168.0.94:6789 192.168.0.99:6789]
              RBDImage:		kubernetes-dynamic-pvc-1a45d656-8226-11e7-a774-005056850b72
              FSType:
              RBDPool:		tenx-pool
              RadosUser:		admin
              Keyring:		/etc/ceph/keyring
              SecretRef:		&{ceph-secret}
              ReadOnly:		false
          Events:			<none>

        #+END_SRC
        从 Type->RBDImage 中可看出，使用的块
        kubernetes-dynamic-pvc-1a45d656-8226-11e7-a774-005056850b72 进行存储的。
        准备工作完成了
****** 更改挂在的 ceph 块大小
       1. 在 ceph 节点上：
        #+BEGIN_SRC sh
         rbd resize --size 20000  kubernetes-dynamic-pvc-1a45d656-8226-11e7-a774-005056850b72
        #+END_SRC
       2. 找到挂在 kubernetes-dynamic-pvc-1a45d656-8226-11e7-a774-005056850b72 对
          应的设备节点，如 kubernetes-dynamic-pvc-1a45d656-8226-11e7-a774-005056850b72
          挂在到在 192.168.0.94 的/dev/rbd3,则需要登录到 94 上，执行一下命令进行扩容：

          #+BEGIN_SRC sh
            resize2fs /dev/rbd3
          #+END_SRC
****** 对 pv 进行扩容
       通过 kubernetes API update 对应的 pv 的 spec->capacity->storage,示例：

       #+BEGIN_SRC yaml
         apiVersion: v1
         kind: PersistentVolume
         metadata:
           annotations:
             pv.kubernetes.io/bound-by-controller: "yes"
             pv.kubernetes.io/provisioned-by: kubernetes.io/rbd
           creationTimestamp: 2017-08-14T07:36:26Z
           name: pvc-47aeb1b2-80c3-11e7-bd70-005056850b72
           resourceVersion: "2616017"
           selfLink: /api/v1/persistentvolumespvc-47aeb1b2-80c3-11e7-bd70-005056850b72
           uid: 47c03480-80c3-11e7-bd70-005056850b72
         spec:
           accessModes:
           - ReadWriteMany
           capacity:
             storage: 1Gi                 # 需要更新的字段
           claimRef:
             apiVersion: v1
             kind: PersistentVolumeClaim
             name: datadir-rediscluster-0
             namespace: default
             resourceVersion: "2594704"
             uid: 47aeb1b2-80c3-11e7-bd70-005056850b72
           persistentVolumeReclaimPolicy: Delete
           rbd:
             image: kubernetes-dynamic-pvc-47b3ea1e-80c3-11e7-a774-005056850b72
             keyring: /etc/ceph/keyring
             monitors:
             - 192.168.0.68:6789
             - 192.168.0.55:6789
             - 192.168.0.94:6789
             - 192.168.0.99:6789
             pool: tenx-pool
             secretRef:
               name: ceph-secret
             user: admin
           storageClassName: tenx-rbd
         status:
           phase: Bound
       #+END_SRC

** FAQ
*** 参考
**** 在 Kubernetes 中使用 Sateful Set 部署 Redis Kubernetes 中文社区
 	  https://www.kubernetes.org.cn/2516.html
**** "Persistent Volumes - Kubernetes"
       https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class
**** pv 介绍 http://www.jianshu.com/p/fda9de00ba5f
**** class
     A PV can have a class, which is specified by setting the storageClassName
     attribute to the name of a StorageClass. A PV of a particular class can
     only be bound to PVCs requesting that class. A PV with no storageClassName
     has no class and can only be bound to PVCs that request no particular
     class.
**** mount options
     You can specify a mount option by using the annotation:
       volume.beta.kubernetes.io/mount-options, A mount option is a string which
       will be cumulatively joined and used while mounting volume to the
       disk.Note that not all Persistent volume types support mount
       options.see:https://kubernetes.io/docs/concepts/storage/persistent-volumes/#mountoptions

**** Error creating rbd image: executable file not found in $PATH · Issue #38923 · kubernetes/kubernetes
 	  https://github.com/kubernetes/kubernetes/issues/38923
**** "AdminSocketConfigObs::init: failed: rbd-clients · Issue #278 · ceph/ceph-ansible"
 	   https://github.com/ceph/ceph-ansible/issues/278

**** "ceph/go-ceph: Go bindings for RADOS, RBD, and CephFS"
 	   https://github.com/ceph/go-ceph
*** kubernetes resize pv 进展
****** Add support for resizing PVs · Issue #284 · kubernetes/features
 	     https://github.com/kubernetes/features/issues/284
****** Allow Updating StatefulSet's entire PodTemplateSpec · Issue #41015 · kubernetes/kubernetes
 	     https://github.com/kubernetes/kubernetes/issues/41015
****** Support \"fstype\" parameter in dynamically provisioned PVs by codablock · Pull Request #45345 · kubernetes/kubernetes
 	     https://github.com/kubernetes/kubernetes/pull/45345/files
****** community/volume-provisioning.md at master · kubernetes/community
 	     https://github.com/kubernetes/community/blob/master/contributors/design-proposals/volume-provisioning.md
****** "Do not release resource(image) when using rbd storageclass. · Issue #45067 · kubernetes/kubernetes"
       *If you delete a PV directly, it won't trigger PV controller to unbind the PV from the PVC and storage backend to remove the image. You need to delete the PVC to delete the rbd image.*
 	     https://github.com/kubernetes/kubernetes/issues/45067
****** "[StatefulSet] Fail to scale pod when there is a blocking pod · Issue #36333 · kubernetes/kubernetes"
 	     https://github.com/kubernetes/kubernetes/issues/36333
****** secret in storageclass "Find a way to pass a secret to dynamic provisioner · Issue #30897 · kubernetes/kubernetes"
 	     https://github.com/kubernetes/kubernetes/issues/30897
       long story short,Ceph's admin secret is only pulled by provision controller. Ceph user's
       secret is used for map,

* 存储测试须知
** rbd
   1. 保证 rbd agent 地址正确，storage-agent 服务正常，storage-agent 可以访问到 rbd 集
      群
   2. 确保 rbd 集群配置正确，其中所填写的 ip 和端口，登录到 ceph 集群任意一台设备上，
      执行 ceph -s， 其中 monmap e3: 3 mons at
      {ceph-node1=192.168.1.86:6789/0,ceph-node2=192.168.1.87:6789/0,ceph-node3=192.168.1.88:6789/0}
      表述 ceph 集群的配置
   3. RBD 认证用户和用户认证秘钥：登录到 ceph 集群的任何设备上，查看文件
      /etc/ceph/ceph.client.admin.keyring，，当前文件存储已认证的用户，如里面的内容如下：
      #+BEGIN_SRC js
        [client.admin]      // admin 为认证用户
        key = AQD/+PhWLE24KhAAO5KslagvWGl/u2FxhGLoqw==   // RBD 用户认证密钥，填写时，只填写 key 的值
      #+END_SRC

** nfs
   1. 根据 wiki：http://wiki.tenxcloud.com/display/Docs/Storage+-+NFS，创建 nfs
      服务，并配置，保证服务可用
   2. 添加 nfs 服务地址，并将配置好的服务挂载路径写入配置中
